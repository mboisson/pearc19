%
% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{acmart}
%\usepackage[french]{babel}
%\selectlanguage{french}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
%
% defining the \BibTeX command - from Oren Patashnik's original BibTeX documentation.
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08emT\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
   
% Rights management information.
% This information is sent to you when you complete the rights form.
% These commands have SAMPLE values in them; it is your responsibility as an author to replace
% the commands and values with those provided to you when you complete the rights form.
%
% These commands are for a PROCEEDINGS abstract or paper.
\copyrightyear{2019}
\acmYear{2019}
\setcopyright{acmlicensed}
\acmConference[PEARC19]{PEARC19: Practice and Experience in Advanced Research Computing}{July 28 -- August 01, 2019}{Chicago, IL}
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection, June 03--05, 2018, Woodstock, NY}
%\acmPrice{15.00}
%\acmDOI{10.1145/1122445.1122456}
%\acmISBN{978-1-4503-9999-9/18/06}

%
% These commands are for a JOURNAL article.
%\setcopyright{acmcopyright}
%\acmJournal{TOG}
%\acmYear{2018}\acmVolume{37}\acmNumber{4}\acmArticle{111}\acmMonth{8}
%\acmDOI{10.1145/1122445.1122456}

%
% Submission ID.
% Use this when submitting an article to a sponsored event. You'll receive a unique submission ID from the organizers
% of the event, and this ID should be used as the parameter to this command.
%\acmSubmissionID{123-A56-BU3}

%
% The majority of ACM publications use numbered citations and references. If you are preparing content for an event
% sponsored by ACM SIGGRAPH, you must use the "author year" style of citations and references. Uncommenting
% the next command will enable that style.
%\citestyle{acmauthoryear}

%
% end of the preamble, start of the body of the document source.
\begin{document}

%
% The "title" command has an optional parameter, allowing the author to define a "short title" to be used in page headers.
\title{Providing A Unified User Environment for Canada National Advanced Computing Centers}

%
% The "author" command and its associated commands are used to define the authors and their affiliations.
% Of note is the shared affiliation of the first two authors, and the "authornote" and "authornotemark" commands
% used to denote shared contribution to the research.
\author{Maxime Boissonneault}
\email{maxime.boissonneault@calculquebec.ca}
\affiliation{%
  \institution{Université Laval, Calcul Québec, Compute Canada}
  \streetaddress{2325 Rue de l'Université}
  \city{Québec}
  \state{Québec}
  \country{Canada}
  \postcode{G1V 0A6}
}
\author{Bart Oldeman}
\email{bart.oldeman@calculquebec.ca}
\affiliation{%
  \institution{McGill University, Calcul Québec, Compute Canada}
  \streetaddress{845 Rue Sherbrooke Ouest}
  \city{Montréal}
  \state{Québec}
  \country{Canada}
  \postcode{H3A 0G4}
}

\author{Ryan P. Taylor}
\email{rptaylor@uvic.ca}
\affiliation{%
  \institution{University of Victoria, WestGrid, Compute Canada}
  \streetaddress{3800 Finnerty Rd}
  \city{Victoria}
  \state{British Columbia}
  \country{Canada}
  \postcode{V8P 5C2}}


%
% By default, the full list of authors will be used in the page headers. Often, this list is too long, and will overlap
% other information printed in the page headers. This command allows the author to define a more concise list
% of authors' names for this purpose.
\renewcommand{\shortauthors}{Boissonneault, Oldeman and Taylor}

%
% The abstract is a short summary of the work to be presented in the article.
\begin{abstract}
Exploiting an advanced computing platform consisting of several clusters distributed across the second-largest country
in the world is challenging. Each cluster may run a different operating system, use a different generation of CPU, GPU,
or network fabric, or be managed by a different team of system administrators. Presenting a unified software
environment can tremendously facilitate the task of supporting researchers, but is difficult to implement. This is
nevertheless what Compute Canada set out to do in 2016, in the midst of deploying a new generation of large clusters.

In order to achieve this goal, we had to find software solutions to solve these challenges. Distribution, portability
and performance were three important technical criteria for us. We also had to consider the practicality of each
approach for our users, and reproducibility of software installations performed by staff located at various sites
across Canada.

In this paper, we present the solution that we created, which has allowed Compute Canada to serve the needs of over
10,000 researchers across the country. It is used on over 20 different clusters with heterogeneous configurations, from
CPU architectures as old as Nehalem or Opteron all the way up to Skylake, with and without GPUs, with InfiniBand,
Ethernet or OmniPath as the network fabric, and with Slurm or Torque/Moab as the scheduler. It presents a unified
software environment to the users, providing over 600 different scientific applications that are available in over
4,000 different combinations of version, compiler and CPU architecture.
\end{abstract}

%
% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
% Please copy and paste the code instead of the example below.
%


%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}
%\ccsdesc{Computer systems organization~Robotics}
%\ccsdesc[100]{Networks~Network reliability}

%
% Keywords. The author(s) should pick words that accurately describe the work being
% presented. Separate the keywords with commas.
\keywords{cvmfs, easybuild, nix, software deployment}

%
% A "teaser" image appears between the author and affiliation information and the body
% of the document, and typically spans the page.
%\begin{teaserfigure}
%  \includegraphics[width=\textwidth]{sampleteaser}
%  \caption{Seattle Mariners at Spring Training, 2010.}
%  \Description{Enjoying the baseball game from the third-base seats. Ichiro Suzuki preparing to bat.}
%  \label{fig:teaser}
%\end{teaserfigure}

%
% This command processes the author and affiliation and title information and builds
% the first part of the formatted document.
%\maketitle

\section{Introduction}
\label{sec:Introduction}
In 2016, Compute Canada set out to refresh its advanced computing infrastructure by deploying a few large clusters to replace nearly 30 smaller legacy clusters. This was a good opportunity to rethink our approach to software installation. Up to this point, each of the legacy clusters required individual manual software installation. This was laborious because each software package had to be installed tens of times, and it created a poor experience for users when moving between clusters, due to inconsistencies in software installations. When we set out to build a common software environment, we first established four guiding principles for our design choices.

The first principle is that software packages should be available on every cluster (unless not permitted because of license or hardware reasons). This means that we need a scalable resilient multi-site distribution mechanism. There are a few solutions that could be used, and we will discuss them in Section~\ref{sec:Distribution_mechanism}. The solution we opted for is Cern Virtual Machine File System, or CVMFS\cite{CVMFS}.

The second principle is that the software environment should be decoupled from the underlying operating system. When we first designed the solution, new clusters were being deployed with the CentOS 7 operating system. However, they did not all have the same set of system packages installed. Moreover, we were interested in supporting legacy clusters that were running CentOS 6, and virtual machines launched by users on one of several Compute Canada cloud platforms. In order to use the unified software environment on virtual machines and multiple clusters, we needed a compatibility layer. The solution we deployed is Nix \cite{Nix}, although we are experimenting with Gentoo Prefix \cite{Gentoo} as a replacement. We will discuss these options in Section~\ref{sec:Compatibility_layer}.

The third principle is that installation procedures should be tracked and reproducible. This means that automation is needed. The solution identified for the compatibility layer already addresses this issue. However, these solutions mostly target basic Linux packages. For scientific packages, we also wanted to ensure that the software packages were optimized for specific hardware architectures. Two tools with a large set of existing recipes were known to us at the time: Spack\cite{Spack} and EasyBuild \cite{EasyBuild2012,EasyBuild2014,EasyBuild2016}. We will discuss why we chose EasyBuild in Section~\ref{sub:Compiled_software_packages}.

The fourth principle is that the interface presented to users should be as familiar as possible, and remain manageable with a very large number of software packages installed. All of our clusters were using a system of environment modules \cite{Modules1991,Modules1996}. However, the traditional systems of modules become unwieldy to handle at the scale of thousands of installed modules. Using a hierarchical module tree, which is possible in a newer implementation of environment modules called Lmod \cite{Lmod}, helps address this problem. We will discuss how our implementation meets the third and fourth principles in Section~\ref{sec:Scientific_applications}.

\section{Distribution mechanism}
\label{sec:Distribution_mechanism}
Installing software packages on a single cluster can be done through a shared file system, such as NFS, Lustre, or GPFS. Anyone who has been working in the field of high performance computing for a number of years knows that parallel file systems are often a bottleneck, especially for the IO pattern caused by the action of launching applications. In addition to that problem, this solution is also a single point of failure, since any failure of that shared file system will block the vast majority of jobs. Another solution for a single cluster is to install binary packages on each node, using a mechanism such as RPM. This solution can however lead to diverging configurations on different nodes, and require an additional layer to keep the configurations synchronized. 

Adding multiple clusters to the problem poses a second synchronization problem. To keep clusters synchronized, there are two strategies : pushing changes to the clusters, or pulling changes from a central location. Both strategies could be implemented with a tool such as {\it rsync}. However, synchronizing directories with over 40 million files and  directories takes a few hours, even with a tool as efficient as {\it rsync}. 

We instead chose to use CERN Virtual Machine File System (CVMFS) \cite{CVMFS}. CVMFS is a file system initially designed to distribute software packages to an array of clusters geographically distributed around the world. It is a resilient file system with no single point of failure, and it delivers files to the clients in a pull fashion: each client pulls file from it as needed. Multiple cache layers, at the level of each node and each cluster, keep network traffic to the central repository to a minimum.

\subsection{CVMFS structure}
\label{sub:CVMFS_structure}
CVMFS is designed similarly to the Network Time Protocol stratum model. There is one source of truth called the stratum 0, where any injection of new software is done.  The stratum 0 is never accessed directly by users or even by the clusters. If it fails, the only immediate consequence is that injection of new files is not possible until it is back online.

Snapshots of the stratum 0 are kept for a while, such that it is possible to go back to a previous version if needed. Files in the repository are compressed, and their hash is computed to allow space saving by avoiding duplication. Hashesalso allows CVMFS clients to quickly invalidate their local cache as needed.

The stratum 0 is replicated to multiple strata 1 servers that are the access points of the clusters to the software environment. They provide redundancy and avoid single points of failure. Files are served to the clusters through HTTP and can be cached locally with multiple proxy servers, protecting against an outage of the wide area network. The closest layer of cache can be configured either to use a local disk on the compute nodes, or on a shared file system on in a diskless environment.

\section{Compatibility layer}
\label{sec:Compatibility_layer}
Isolating the software environment from the operating system requires a compatibility layer containing almost everything that an OS would contain. From this layer, we exclude the Linux kernel and its modules, hardware drivers as well as privileged binaries.

\subsection{Nix}
\label{sub:Nix}
Nix \cite{Nix} is both a package manager and an operating system (NixOS). Reproducibility being one the core principles in Nix, it is designed to depend as little as possible on the underlying operating system. Every software package installed through nix resides in a path calculated with the hash of the recipe to build it. This ensures very strong reproducibility, but it also makes updating a package in place impossible. While reproducibility is an important consideration in advanced research computing, bugs sometimes need to be fixed, which is difficult due to Nix' design. At the time of writing this paper, we are still using Nix as a compatibility layer, but we are investigating a switch to Gentoo Prefix described next.

\subsection{Gentoo Prefix}
\label{sub:Gentoo_Prefix}
Gentoo Prefix~\cite{Gentoo} is an alternative solution to provide a compatibility layer. It allows installation of most packages that make a typical Linux inside a non-default location. Contrary to Nix however, packages are installed in a standard directory structure. This is helpful when updates are required, and minimizes problems with applications that expect a standard directory structure. At the time of this writing, we are still experimenting with Gentoo Prefix, and it is not used in production. 

\section{Scientific applications}
\label{sec:Scientific_applications}

\subsection{Compiled software packages}
\label{sub:Compiled_software_packages}
Scientific applications constitute the majority of our software stack. Since their performance is critical, they can not be installed in the compatibility layer, as they need to be optimized for specific CPU instructions. To install them, we use the EasyBuild~\cite{EasyBuild2012,EasyBuild2014,EasyBuild2016} package manager.

EasyBuild originated from University of Ghent, but has now over 200 contributors from all around the world. The framework is based on scripted recipes to install packages, usually from source code. It is designed to create optimized binaries, and eases recompilation of multiple versions of a given application using many combinations of compilers or libraries.

EasyBuild also automatically creates module files~\cite{Modules1991,Modules1996,Lmod}. The framework supports multiple standard and custom naming schemes. We use a version of a hierarchical naming scheme, in which modules are organized in a tree structure, hiding those that are incompatible currently loaded modules. This limits the list of modules which could otherwise be very long. This can however throw off some users, since they don’t automatically see all available modules. Lmod~\cite{Lmod} alleviates this issue by providing a command to crawl the full module tree and instructs users on how to load the module they want.

\subsection{Handling multiple CPU architectures}
\label{sub:Handling_multiple_CPU_architectures}
Handling multiple CPU architectures is done by installing applications in architecture dependent paths and module trees. We also have a wide selection of software packages that are installed in an architecture-independent fashion. This is the case, for example, for binary packages, packages that are not performance critical, or packages that consists of source files only. The same recipe is typically used for installing the same package for multiple architectures. While each cluster will usually show only a single architecture, users are allowed to select a different architecture if they wish to run exactly the same binary as on another cluster.

\subsection{Packages for scripting languages}
\label{sub:Packages_for_scripting_languages}
For scripting languages that support installing packages, such as Python, R or Perl, we leverage the existing package manager of that language, rather than installing them as modules. In the case of Python, we are building Python wheels~\cite{Wheels}, linking against our software stack. Wheels are binary packages that can be installed in a matter of seconds, similar to CentOS RPM packages. Since they are built by our team, and linked against our optimize software stack, we ensure that they are both easy to installed and offer good performance.

\subsection{Licensed and restricted software packages}
\label{sub:Licensed_software_packages}
Compute Canada also has agreements with a number of software vendors or developers of applications that require a license. In these cases, we install the package using the same tools, but it is deployed on a separate CVMFS repository which is only accessible to Compute Canada clusters.

\section{Other benefits}
\label{sec:Other_benefits}
Because of the portability of our software stack and the distribution mechanism, it can be used on any cluster, or any virtual machine on a cloud. For example, for training events, we have been using it to provision small sized replicas of Compute Canada clusters in short-lived virtual clusters deployed on our OpenStack platform. We have also used this portable software environment to expose a cluster environment directly through a browser, via JupyterHub. It can also help support infrastructures owned by various institutions.

\section{Acknowledgments}
\label{sec:Acknowledgments}
We thank the following organizations and individuals for sharing their time and expertise:
\begin{itemize}
	\item All of the Compute Canada team members who make it possible to manage such a large software stack
	\item Robert McLay, from Texas Advanced Computing Center, for always providing quick and efficient support and
debugging for Lmod
	\item Kenneth Hoste from Ghent University, and Åke Sandgren from Umeå University, for many discussions
regarding EasyBuild
	\item Kuang Chung Chen and Bryan Caron, from McGill University, for initiating the groundwork that lead to this
project
	\item Dave Dykstra, Jakob Blomer, and the CernVM project team for support and development of CVMFS
\end{itemize}

%
% The next two lines define the bibliography style to be used, and the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{biblio}


\end{document}
