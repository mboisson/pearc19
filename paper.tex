%
% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{acmart}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
%
% defining the \BibTeX command - from Oren Patashnik's original BibTeX documentation.
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08emT\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
   
% Rights management information.
% This information is sent to you when you complete the rights form.
% These commands have SAMPLE values in them; it is your responsibility as an author to replace
% the commands and values with those provided to you when you complete the rights form.
%
% These commands are for a PROCEEDINGS abstract or paper.
\copyrightyear{2019}
\acmYear{2019}
\setcopyright{acmlicensed}
\acmConference[PEARC19]{PEARC19: Practice and Experience in Advanced Research Computing}{July 28 -- August 01, 2019}{Chicago, IL}
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection, June 03--05, 2018, Woodstock, NY}
%\acmPrice{15.00}
%\acmDOI{10.1145/1122445.1122456}
%\acmISBN{978-1-4503-9999-9/18/06}

%
% These commands are for a JOURNAL article.
%\setcopyright{acmcopyright}
%\acmJournal{TOG}
%\acmYear{2018}\acmVolume{37}\acmNumber{4}\acmArticle{111}\acmMonth{8}
%\acmDOI{10.1145/1122445.1122456}


%
% Submission ID.
% Use this when submitting an article to a sponsored event. You'll receive a unique submission ID from the organizers
% of the event, and this ID should be used as the parameter to this command.
%\acmSubmissionID{123-A56-BU3}

%
% The majority of ACM publications use numbered citations and references. If you are preparing content for an event
% sponsored by ACM SIGGRAPH, you must use the "author year" style of citations and references. Uncommenting
% the next command will enable that style.
%\citestyle{acmauthoryear}

%
% end of the preamble, start of the body of the document source.
\begin{document}

%
% The "title" command has an optional parameter, allowing the author to define a "short title" to be used in page headers.
\title{Providing a Unified Software Environment for Canada’s National Advanced Computing Centers}

%
% The "author" command and its associated commands are used to define the authors and their affiliations.
% Of note is the shared affiliation of the first two authors, and the "authornote" and "authornotemark" commands
% used to denote shared contribution to the research.
\author{Maxime Boissonneault}
\email{maxime.boissonneault@calculquebec.ca}
\affiliation{%
  \institution{Université Laval, Calcul Québec, Compute Canada}
  \streetaddress{2325 Rue de l'Université}
  \city{Québec}
  \state{Québec}
  \country{Canada}
  \postcode{G1V 0A6}
}
\author{Bart Oldeman}
\email{bart.oldeman@calculquebec.ca}
\affiliation{%
  \institution{McGill University, Calcul Québec, Compute Canada}
  \streetaddress{845 Rue Sherbrooke Ouest}
  \city{Montréal}
  \state{Québec}
  \country{Canada}
  \postcode{H3A 0G4}
}

\author{Ryan P. Taylor}
\email{rptaylor@uvic.ca}
\affiliation{%
  \institution{University of Victoria, WestGrid, Compute Canada}
  \streetaddress{3800 Finnerty Rd}
  \city{Victoria}
  \state{British Columbia}
  \country{Canada}
  \postcode{V8P 5C2}}


%
% By default, the full list of authors will be used in the page headers. Often, this list is too long, and will overlap
% other information printed in the page headers. This command allows the author to define a more concise list
% of authors' names for this purpose.
\renewcommand{\shortauthors}{Boissonneault, Oldeman and Taylor}

%
% The abstract is a short summary of the work to be presented in the article.
\begin{abstract}
Exploiting an advanced computing platform consisting of several clusters distributed across the second-largest country in the world is challenging. Each cluster may run a different operating system, use a different generation of CPU, GPU, or network fabric, or be managed by a different team of system administrators. Presenting a unified software environment can tremendously facilitate the task of supporting researchers, but is challenging to implement. This is nevertheless what Compute Canada set out to do in 2016, in the midst of deploying a new generation of large clusters. 

We had to find software solutions to solve the challenges involved to achieve this goal. Distribution, portability and performance were three important technical criteria for us. We also had to consider the practicality of each approach for our users, and reproducibility of software installations performed by staff located at various sites across Canada. 

In this paper, we present the solution that we created, which has allowed Compute Canada to serve the needs of over 10,000 researchers across the country. This solution is used on over 20 different clusters with heterogeneous configurations, on processor architectures ranging from AMD's 2010 Magny-Cours to Intel's 2017 Skylake SP, with or without GPUs, with InfiniBand, Ethernet or OmniPath as the network fabric, and with Slurm or Torque/Moab as the scheduler. This stack provides a unified software environment to users, providing over 600 different scientific applications that are available in over 4,000 different combinations of version, compiler and CPU architecture.
\end{abstract}

%
% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
% Please copy and paste the code instead of the example below.
%


%\ccsdesc[500]{Computer systems organization~Embedded systems}
%\ccsdesc[300]{Computer systems organization~Redundancy}
%\ccsdesc{Computer systems organization~Robotics}
%\ccsdesc[100]{Networks~Network reliability}

%
% Keywords. The author(s) should pick words that accurately describe the work being
% presented. Separate the keywords with commas.
\keywords{cvmfs, easybuild, nix, software deployment}

%
% A "teaser" image appears between the author and affiliation information and the body
% of the document, and typically spans the page.
%\begin{teaserfigure}
%  \includegraphics[width=\textwidth]{sampleteaser}
%  \caption{Seattle Mariners at Spring Training, 2010.}
%  \Description{Enjoying the baseball game from the third-base seats. Ichiro Suzuki preparing to bat.}
%  \label{fig:teaser}
%\end{teaserfigure}

%
% This command processes the author and affiliation and title information and builds
% the first part of the formatted document.
\maketitle

\section{Introduction}
\label{sec:Introduction}
In 2016, Compute Canada set out to refresh its advanced computing infrastructure by deploying a few large clusters to replace nearly 30 smaller legacy clusters. This was a good opportunity to rethink our approach to software installation. Up to this point, each of the legacy clusters required individual manual software installation. This was laborious because each software package had to be installed dozens of times, and it created a poor experience for users when moving between clusters, due to inconsistencies in software installations. When we set out to build a common software environment, we first established four guiding principles for our design choices.

The first principle is that software packages should be available on every cluster (unless not possible because of license or hardware reasons). This meant that we needed a scalable resilient multi-site distribution mechanism. There are a few solutions that could be used, and we will discuss them in Section~\ref{sec:Distribution_mechanism}. The solution we opted for is CERN Virtual Machine File System, or CVMFS\cite{CVMFS}.

The second principle is that the software environment should be decoupled from the underlying operating system. When we first designed the solution, new clusters were being deployed with the CentOS 7 operating system. However, they did not all have the same set of system packages installed. Moreover, we were interested in supporting legacy clusters
that were running CentOS 6, and virtual machines launched by users on several Compute Canada cloud platforms. In order to use the unified software environment on all of these systems, we needed a compatibility layer. 
The solution we deployed is Nix \cite{Nix}, although we are experimenting with Gentoo Prefix \cite{Gentoo} as a replacement. We will discuss these options in Section~\ref{sec:Compatibility_layer}.

The third principle is that installation procedures should be tracked and reproducible. This means that automation is needed. Nix already addresses this issue; however it mostly targets basic Linux packages. For scientific packages, we also wanted to ensure that the software packages were optimized for specific hardware architectures, which Nix cannot do easily. The tool we chose for this task is EasyBuild~\cite{EasyBuild2012,EasyBuild2014,EasyBuild2016}, which we will discuss in Section~\ref{sub:Compiled_software_packages}.

The fourth principle is that the interface presented to users should be as familiar as possible, and remain manageable with a very large number of software packages installed. All of our clusters were using a system of environment modules \cite{Modules1991,Modules1996}. However, the traditional module systems become unwieldy at the scale of thousands of installed modules. Using a hierarchical module tree, which is possible in a newer implementation of environment modules called Lmod \cite{Lmod}, helps address this problem. We will discuss how our implementation meets the third and fourth principles in Section~\ref{sec:Scientific_applications}.

\section{Distribution mechanism}
\label{sec:Distribution_mechanism}
A shared file system such as NFS, Lustre, or GPFS, can provide access to software packages on a single cluster. However, experience in the field of high performance computing shows that parallel file systems are often challenging to operate at scale in a reliable and performant manner, especially for the high I/O operations per second caused by application startups. In addition, a shared file system is a single point of failure for a cluster, which nearly all jobs rely on in order to run. An alternative for a single cluster is to install binary packages on each node, using a mechanism such as RPM. However, this can result in divergent configurations across nodes, may be limited by available space on the local disk, and requires an additional configuration management layer to keep the packages synchronized. 

For a national platform spanning multiple clusters, latency precludes a parallel file system from consideration, and the synchronization problem is compounded. The clusters could be synchronized by pulling or pushing changes with a tool such as {\it rsync}, but this would not be scalable; even a tool as efficient as {\it rsync} takes several hours to synchronize over 40 million files and directories. Moreover, this method of copying files is not transactional or atomic, so an additional indirection layer would be required to provide a complete solution.

We therefore chose to use CERN Virtual Machine File System (CVMFS) \cite{CVMFS}. CVMFS is a file system designed to distribute software packages to clusters at a global scale with no single point of failure. Clients access files by pulling content on demand, using multiple levels of caching, to minimize network traffic and storage use and ensure responsive and scalable performance.

We will describe the generic structure of a CVMFS deployment in Section~\ref{sub:CVMFS_structure}, and describe the deployment specific to Compute Canada in Section~\ref{sub:CVMFS_in_CC}

\subsection{CVMFS structure}
\label{sub:CVMFS_structure}
CVMFS is designed similarly to the Network Time Protocol stratum model, as illustrated in Figure~\ref{fig:CVMFS_structure}. There is one source of truth called the stratum 0, where software is injected.  The stratum 0 is never accessed directly by clients. If it fails, the only immediate consequence is that injection of new files is not possible until it is back online. 

Snapshots of the stratum 0 are kept for a while, making it possible to roll back to a previous version if needed. Files in the repository are compressed, and their hash is computed to save space by deduplicating objects. Hashes also allow CVMFS clients to quickly verify and invalidate their local caches when content is changed.

The stratum 0 is replicated to multiple strata 1 servers, which serve content to clients and provide redundancy and load balancing. Files are served to the clusters via HTTP, and each cluster has local caching proxy servers to reduce the load placed on the stratum 1 servers by its worker nodes, and mitigate the impact of a wide area network outage. The closest layer of cache can be configured to use a local disk on the compute nodes, or a shared file system in a diskless environment.

\textbf{TODO: Insert figure representing the CVMFS structure of strata. }
\label{fig:CVMFS_structure}

\subsection{Deployment of CVMFS in Compute Canada}
\label{sub:CVMFS_in_CC}
For Compute Canada's needs, we have deployed three CVMFS repositories, each composed of a stratum 0 and one or multiple strata 1. This allows us to follow a workflow that includes testing before exposing software packages to user, as illustrated in Fig~\ref{fig:Workflow}.

First, we have a development repository. This repository is used to deploy software from our build infrastructure onto the clusters without exposing it to end users. This allows us test newly installed software packages on our clusters without impacting production. This minimizes the odds of a faulty package being pushed to production. This is especially important when a bug is found in one of the software packages and we need to update it. This also happens for example if we need to make minor changes to an MPI implementation in order to correctly support a new cluster with a different networking fabric. 

We then have a production repository. By design, software packages are always copied onto the production repository from the development repository, and never from the build infrastructure directly. This workflow also minimizes the odds of a faulty software making its way to the end users. This repository holds all of the software environment which is visible to the end users on all of our clusters. Once a piece of software is deployed onto this repository, it becomes visible on all clusters within 30 minutes. 

Both repositories above are available publicly, and can be mounted on any cluster or virtual machine in the cloud. We also have a third repository which is used to host software packages that are restricted to Compute Canada due to license limitations. We work with software vendors and developers in order to get the authorization to install licensed and commercial software packages in this repository. The most common licensing model that we use is a BYOL (bring your own license) model, in which we provide the software package, but end users provide a license server to connect to. Another model that is used is to control access to a software package via POSIX groups. By default, CVMFS clients do not honour POSIX group and filesystem permissions, but they can be configured to do so. Since this kind of restriction requires a specific LDAP and a specific configuration, this repository is restricted and can only be made available to clusters that are run by Compute Canada teams. 

One final use case for CVMFS is to hold and distribute datasets. However, default CVMFS configuration is optimized for a large number of small files, rather than a small number of large files. This means that a special configuration must be done for this repository. At the time of writing this paper, this use case is still experimental within Compute Canada, but other groups are known to use CVMFS in this way.

\textbf{TODO: INSERT FIGURE representing our workflow (build => test locally => dev => test on cluster => prod) }
\label{fig:Workflow}


\section{A layered approach}
\label{sec:Layered_environment}
To install software packages for our common environment, we follow a layered approach, as illustrated in Table~\ref{tab:layers}. First, we require a basic Linux installation. Any version of Linux with a kernel 2.6.32 or more recent should work. This layer is managed by the system administrators, and should contain any driver, such as GPU drivers or drivers for the network fabric, kernel modules, as well as privileged binaries. 

The second layer is what we call our compatibility layer. It contains almost all packages that one would typically install with the Linux distribution's package manager, such as {\it yum} or {\it apt-get}. 

The third layer is the scientific application layer. It contains mostly scientific or performance critical applications, including for example MPI implementations, CUDA, mathematical libraries, and domain specific applications. 

\textbf{TODO: INSERT FIGURE/TABLE WITH OS layer, Nix layer, EasyBuild layer}
\label{tab:layers}


\section{Compatibility layer}
\label{sec:Compatibility_layer}
Isolating the software environment from the operating system requires a compatibility layer containing almost everything that an OS would provide. We exclude the Linux kernel and its modules, hardware drivers, and privileged binaries from this layer.

\subsection{Nix}
\label{sub:Nix}
Nix~\cite{Nix} is both a package manager and an operating system (NixOS). One of the core principles in Nix is reproducibility, so it is designed to depend as little as possible on the underlying operating system. 

The installation path of each software package installed through Nix resides in a path called the Nix store. This stores contains a single directory for each version of each software package installed through Nix. These directories are named after a hash calculated from the installation recipe and its dependencies. This creates a very large number of subdirectories, many of which contain almost identical files. 

In addition to the store, Nix creates directories that are called profiles. These profile directories contain a farm of symbolic links that point to files in the Nix store. For example, it will contain a {\it bin} directory, which will contain symbolic links to typical operating system commands such as {\it ls} or {\it gcc}, which will reside in different paths in the store. It will also contain {\it lib} and {\it include} directories which will contain symbolic links to libraries and header files located in the store. 

Profiles are the paths that are exposed to the end users, which is done by adding paths to their {\it PATH}, {\it CPATH} and other similar environment variables. Each profile ever created is kept, but users are only exposed to the most recent one. Each profile represents the complete state of the user-exposed environment at a given time in the past.

Nix has a mechanism for garbage collection, which can be used to delete packages that are no longer used in the current profile. It also has a mechanism to resolve the complete list of files from the Nix store that are required to compose a given profile. Because Nix generates so many files, we use this mechanism to deploy on CVMFS only those files that are used in the current profile. 

This works well until a software exposes paths to the Nix store directly, instead of the paths to Nix profiles. We call this a Nix store {\it leak}. This happens for example with Python when using virtual environments. Virtual environments in Python copy the {\it python} executable to a secondary location, outside of Nix. However, all binaries installed by Nix use {\it RPATH} for shared object resolutions in the store directly. This means that copying a binary outside of Nix creates a dependency between a binary that now resides outside of the store, and the store itself. Because Nix is unaware of that dependency, any update of one of the dependencies followed by garbage collection or selective deployment of files will end up breaking this binary. We have encountered this issue with Python, Perl and Qt, which we have therefore moved outside of Nix and installed them through EasyBuild instead.

Another issue that we have encountered is that many Nix recipes call for installing sub-packages of a package in separate directories. For example, for Qt, {\it qmake} and {\it OTHER COMMAND} reside in different directories in the Nix store. This sometimes cause issues when installing a dependent package which assumes that its dependencies are installed in a more traditional fashion, with all of the binaries in the same {\it bin} folder, alongside with headers in the {\it include} folder, and libraries in the {\it lib} folder. This has mostly been an issue for Qt. 


\subsection{Gentoo Prefix}
\label{sub:Gentoo_Prefix}
Gentoo Prefix~\cite{Gentoo} is an alternative solution to provide a compatibility layer. It allows installation of most packages that form a typical Linux operating system in a non-default location. Contrary to Nix however, packages are installed in a standard directory structure ({\it bin}, {\it lib}, {\it include}, {\it share}). This is helpful when updates are required, and minimizes problems with applications that expect a standard directory structure. At the time of this writing, we are still experimenting with Gentoo Prefix, and it is not used in production. 

\section{Scientific applications}
\label{sec:Scientific_applications}

\subsection{Compiled software packages}
\label{sub:Compiled_software_packages}
Scientific applications constitute the majority of our software stack. Since their performance is critical, they can not be installed in the compatibility layer, as they need to be optimized for specific CPU instructions. To install them, we use the EasyBuild~\cite{EasyBuild2012,EasyBuild2014,EasyBuild2016} software installation tool.

EasyBuild originated from the University of Ghent, but now has over 200 contributors from around the world. The framework is based on scripted recipes (in Python) to install packages, usually from source code. It is designed to create optimized binaries, and eases recompilation of multiple versions of a given application using many combinations of compilers or libraries. EasyBuild is composed of three parts: the framework, the EasyBlocks, and the EasyConfigs. 

The framework contains all of the core code which drives the logic of installing a software. It drives all of the steps necessary to install a software: parsing the configuration options, downloading source files, patching them, configuring, building and installing the software, as well as performing post-installation steps such as sanity checks, generating a module or running custom commands. 

Written in Python, EasyBlocks define what needs to be done at each step for a given software package. There are generick EasyBlocks, such as {\it ConfigureMake} or {\it CMakeMake}, as well as custom EasyBlocks for packages that have more complicated installation procedures, such as OpenFOAM or PETSc. 

EasyConfigs files parametrize the execution of the code from the EasyBlocks. They define the software name and version, the URL from which the source files can be downloaded as well as options to be used for the various steps. 

In most cases, there is already an EasyBlock for a given software package, or it can be installed easily with one of the generic EasyBlocks. In these cases, writing a new recipe is as simple as editing a configuration file. EasyBuild also supports command line options to reuse and existing EasyConfig file, while changing the software version or the toolchain being used. 

Toolchains are a core concept in EasyBuild. They represent a set or subset of a compiler, and MPI implementation, mathematical libraries and CUDA. Having the ability to generate multiple builds of a given software simply by providing a command line argument to change the toolchain has proven unvaluable to our workflow, and has allowed us to recompile hundreds of packages in matter of days when deploying a new version of compiler or a cluster with a new CPU architecture. 

EasyBuild also automatically creates module files~\cite{Modules1991,Modules1996,Lmod}. The framework supports multiple standard and custom naming schemes. We use a version of a hierarchical naming scheme, in which modules are organized in a tree structure, hiding those that are incompatible with currently loaded modules. This limits the list of visible modules which could otherwise be very extensive. This can however throw off some users, since they don’t automatically see all available modules. Lmod~\cite{Lmod} alleviates this issue by providing a separate command to crawl the full module tree and instructs users on how to load the module they want.

\subsection{Handling multiple CPU architectures}
\label{sub:Handling_multiple_CPU_architectures}
Handling multiple CPU architectures is done by installing applications in architecture-dependent paths and module trees. We also have a wide selection of software packages that are installed in an architecture-independent fashion. This is the case, for example, for binary packages, packages that are not performance critical, or packages that consist of source files only. The same recipe is typically used for installing the same package for multiple architectures. While each cluster will usually show only a single architecture, users are allowed to select a different architecture if they wish to run exactly the same binary as on another cluster.

\subsection{Packages for scripting languages}
\label{sub:Packages_for_scripting_languages}
For scripting languages that support installing packages, such as Python, R or Perl, we leverage the existing package manager of that language, rather than installing them as modules. In the case of Python, we are building Python wheels~\cite{Wheels}, linking against our software stack. Wheels are binary packages that can be installed in a matter of seconds, similar to CentOS RPM packages. Since they are built by our team, and linked against our optimized software stack, we ensure that they are easy to install and offer good performance.

\subsection{Licensed and restricted software packages}
\label{sub:Licensed_software_packages}
Compute Canada also has agreements with a number of software vendors or developers of applications that require a license. In these cases, we install the package using the same tools, but it is deployed on a separate CVMFS repository which is only accessible to Compute Canada clusters.

\section{Other benefits}
\label{sec:Other_benefits}
Because of the portability of our software stack and the distribution mechanism, it can be used on any cluster, or any virtual machine on a cloud. For example, for training events, we have been using it to provision small replicas of Compute Canada clusters in short-lived virtual clusters deployed on our OpenStack platform. We have also used this portable software environment to expose a cluster environment directly through a browser, via JupyterHub. It can also help support infrastructures owned by various institutions.

\section{Acknowledgments}
\label{sec:Acknowledgments}
We thank the following organizations and individuals for sharing their time and expertise:
\begin{itemize}
	\item All of the Compute Canada team members who make it possible to manage such a large software stack
	\item Robert McLay, from Texas Advanced Computing Center, for always providing quick and efficient support and
debugging for Lmod
	\item Kenneth Hoste from Ghent University, and Åke Sandgren from Umeå University, for many discussions
regarding EasyBuild
    \item All of the EasyBuild maintainers and contributors, without which EasyBuild would not have nearly as many supported recipes.
	\item Kuang Chung Chen and Bryan Caron, from McGill University, for initiating the groundwork in Compute Canada that lead to this project
	\item Dave Dykstra, Jakob Blomer, and the CernVM project team for support and development of CVMFS
\end{itemize}

%
% The next two lines define the bibliography style to be used, and the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{biblio}


\end{document}
